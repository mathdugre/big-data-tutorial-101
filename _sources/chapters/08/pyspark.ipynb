{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this section is a jupyter book implementation of [this tutorial](https://github.com/azazel7/BigData-Lab/tree/master/lab2-spark).\n",
    "\n",
    "**Spark Cluster**</br>\n",
    "![Spark Cluster](../../figures/spark-cluster.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install PySpark on your system, I recommend following [this guide](https://www.datacamp.com/tutorial/installation-of-pyspark)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed DAtasets (RDDs) can be seen as a sequence of elements. However, transformation on RDDs are evaluated lazily.\n",
    "</br>The [official RDDs documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html) covers extensively their usage.\n",
    "</br>Cheat Sheet: [PySpark RDDs](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a `SparkContext` to interact with our local Spark Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:47:02 WARN Utils: Your hostname, WIN10 resolves to a loopback address: 127.0.1.1; using 172.31.186.28 instead (on interface eth0)\n",
      "23/01/30 20:47:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:47:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize file content or sequence of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trees.csv MapPartitionsRDD[113] at textFile at DirectMethodHandleAccessor.java:104"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"trees.csv\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[114] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([i for i in range (10)])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark actions evaluate the rdd and usually returns data.\n",
    "</br>Some useful actions:\n",
    "- `collect()` : This function will collect the RDD into a python list.\n",
    "- `take(k)` : This function create an RDD from k element within the RDD.\n",
    "- `count()` : This function returns the number of element within the RDD.\n",
    "\n",
    "Refer to the pyspark documentation for a [complete list of actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tree,Park Name,x,y', '1,Canada Park,2,3', '2,Otter Park,63,21']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark transformation lazily apply a function to a RDD.</br>\n",
    "Refer to the pyspark documentation for a [complete list of transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map(func)` : Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap(func)` : Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 9, 27, 25, 125, 49, 343, 81, 729]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: [x*x, x*x*x] if x % 2 == 1 else []).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter(func)` : Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 7]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x % 3 == 1).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey(func)` : When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 18), (1, 12), (2, 15)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(i%3, i) for i in range(10)]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.reduceByKey(lambda x, y: x+y).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`intersection(dataset)` : Return a new RDD that contains the intersection of elements in the source dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = [i for i in range(10)]\n",
    "data2 = [i for i in range(5, 15)]\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`union(func)`: Return a new dataset that contains the union of the elements in the source dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = [i for i in range(10)]\n",
    "data2 = [i for i in range(5, 15)]\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distinct()` : Return a new dataset that contains the distinct elements of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i%3 for i in range(10)]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`zipWithIndex()` : Assign an index to each element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i for i in range(10)]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.zipWithIndex().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupByKey()` : When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x7f6f6d00e5c0>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x7f6f6d00d810>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x7f6f6d00d9f0>)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(i%3, i) for i in range(10)]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark dataframes are tabular data. Similar to tables in SQL or spreadsheets.\n",
    "| Tree | Park Name   | x  | y  |\n",
    "|------|-------------|----|----|\n",
    "| 1    | Canada Park | 2  | 3  |\n",
    "| 2    | Otter Park  | 63 | 21 |\n",
    "| 3    | Canada Park | 2  | 3  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a Spark session to interact with our local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a csv file into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Tree: string, Park Name: string, x: int, y: int]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"trees.csv\"\n",
    "df = spark.read.csv(filename, header=True, mode=\"DROPMALFORMED\")\n",
    "df = df .withColumn(\"x\", df.x.cast(\"int\")) \\\n",
    "        .withColumn(\"y\", df.y.cast(\"int\"))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`show()` : Display the first rows of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+---+\n",
      "|Tree|  Park Name|  x|  y|\n",
      "+----+-----------+---+---+\n",
      "|   1|Canada Park|  2|  3|\n",
      "|   2| Otter Park| 63| 21|\n",
      "|   3|Canada Park|  2| 25|\n",
      "+----+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select(column name, another column name)` : Return a dataframe with only the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Park Name='Canada Park', Tree='1'),\n",
       " Row(Park Name='Otter Park', Tree='2'),\n",
       " Row(Park Name='Canada Park', Tree='3')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Park Name\", \"Tree\").take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`where(\"action\")` : Return a dataframe where only the condition applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(\"x = 2\").count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupBy()`: Group rows together using aggregate function such as min, max or avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=63, count=1), Row(x=2, count=2)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"x\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|  x|sum(x)|sum(y)|\n",
      "+---+------+------+\n",
      "| 63|    63|    21|\n",
      "|  2|     4|    28|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"x\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  x|avg(y)|\n",
      "+---+------+\n",
      "| 63|  21.0|\n",
      "|  2|  14.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"x\").mean(\"y\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`orderBy(\"column1\", \"column2\", ...)`: Order the dataframe with one or more column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+---+\n",
      "|Tree|  Park Name|  x|  y|\n",
      "+----+-----------+---+---+\n",
      "|   2| Otter Park| 63| 21|\n",
      "|   1|Canada Park|  2|  3|\n",
      "|   3|Canada Park|  2| 25|\n",
      "+----+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.orderBy(desc(\"Park Name\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`limit(number)`: Drop all rows after the number th row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+---+\n",
      "|Tree|  Park Name|  x|  y|\n",
      "+----+-----------+---+---+\n",
      "|   1|Canada Park|  2|  3|\n",
      "|   2| Otter Park| 63| 21|\n",
      "+----+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(2).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`createDataFrame(rdd)`: Create a dataframe from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+---+\n",
      "|Tree|  Park_Name|  x|  y|\n",
      "+----+-----------+---+---+\n",
      "|   1|Canada Park|  2|  3|\n",
      "|   2| Otter Park| 63| 21|\n",
      "|   3|Canada Park|  2| 25|\n",
      "+----+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "lines = sc.textFile(\"trees.csv\")\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda l: l != header)  # Remove header\n",
    "lines = lines.map(lambda x: x.split(\",\"))\n",
    "trees_rdd = lines.map(lambda p: Row(Tree=p[0], Park_Name=p[1], x=p[2], y=p[3]))\n",
    "trees_df = spark.createDataFrame(trees_rdd)\n",
    "trees_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toDF(name1, name2, name3)`: Rename the columns of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+----+\n",
      "| ID|  Park Name|lat|long|\n",
      "+---+-----------+---+----+\n",
      "|  1|Canada Park|  2|   3|\n",
      "|  2| Otter Park| 63|  21|\n",
      "|  3|Canada Park|  2|  25|\n",
      "+---+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees_df = trees_df.toDF(\"ID\", \"Park Name\", \"lat\", \"long\")\n",
    "trees_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`join()`: Combine dataframe based on common column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+---+-----------+---+----+\n",
      "|Tree|  Park Name|  x|  y|  Park Name|lat|long|\n",
      "+----+-----------+---+---+-----------+---+----+\n",
      "|   1|Canada Park|  2|  3|Canada Park|  2|   3|\n",
      "|   2| Otter Park| 63| 21| Otter Park| 63|  21|\n",
      "|   3|Canada Park|  2| 25|Canada Park|  2|  25|\n",
      "+----+-----------+---+---+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(trees_df, df[\"Tree\"] == trees_df.ID).drop(\"ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-tutorial-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7aed4e4d455db0df64e73da1eb387dfb34ca30e0f855d7b7790daae2643b7068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
